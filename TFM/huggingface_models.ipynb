{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:45:06.466852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 10:45:06.518831: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-13 10:45:06.818814: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-07-13 10:45:06.818849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-07-13 10:45:06.818852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar el dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset = datasets.load_dataset(\"recipe_nlg\", data_dir='/workspace/datasets/recipe_nlg')\n",
    "\n",
    "print(recipe_dataset)\n",
    "# Convertir los datos a dataframe de pandas\n",
    "recipe_dataset.set_format(type='pandas')\n",
    "df_recipe_dataset=recipe_dataset['train'][:]\n",
    "\n",
    "# Mostrar la cabecera del dataframe\n",
    "df_recipe_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acondicionamiento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar las columnas \"title\", \"ingredients\" y \"directions\" para tener la receta completa como un texto, los \"ner\" igual\n",
    "full_recipe=[]\n",
    "Ingredients=[]\n",
    "for row in range(df_recipe_dataset.shape[0]):\n",
    "    ingredients = \"\\n\".join(df_recipe_dataset['ner'][row])\n",
    "    directions = \"\\n\".join(df_recipe_dataset['directions'][row])\n",
    "    Ingredients.append('ingredients:\\n'+ingredients.lower())\n",
    "    full_recipe.append('<|startofrecipe|>ingredients:\\n'+ingredients.lower()+'\\ndirections:\\n'+directions.lower()+'<|endofrecipe|>')\n",
    "\n",
    "df_recipe_dataset['Full Recipe']=full_recipe\n",
    "df_recipe_dataset['Ingredients']=Ingredients\n",
    "# Eliminar columnas que no son útiles\n",
    "df_recipe_dataset=df_recipe_dataset.drop(['link', 'source','title','directions','ingredients','id','ner'], axis=1)\n",
    "\n",
    "# Visualizar el dataset acondicionado\n",
    "print(\"DATASET ACONDICIONADO:\")\n",
    "display(df_recipe_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/workspace/datasets/clean_recipe_nlg/\"):\n",
    "    os.mkdir(\"/workspace/datasets/clean_recipe_nlg/\")\n",
    "df_recipe_dataset.iloc[:100,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_100_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:1000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_1000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:10000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_10000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:100000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_100000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:1000000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_1000000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_full.csv\", sep=\";\")\n",
    "del df_recipe_dataset\n",
    "del recipe_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe_dataset = pd.read_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_full.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Full Recipe', '__index_level_0__'],\n",
      "        num_rows: 1784913\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['Unnamed: 0', 'Full Recipe', '__index_level_0__'],\n",
      "        num_rows: 446229\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convertir a Dataset de HuggingFace\n",
    "recipe_dataset = Dataset.from_pandas(df_recipe_dataset.sample(frac=1))\n",
    "\n",
    "# Dividir el dataset en train y test de forma aleatoria\n",
    "recipe_dataset_train_valid = recipe_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"DATASET:\")\n",
    "recipe_dataset = DatasetDict({\n",
    "    'train': recipe_dataset_train_valid['train'],\n",
    "    'valid': recipe_dataset_train_valid['test'],\n",
    "})\n",
    "print(recipe_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)\n",
    "#special_tokens_dict = {'additional_special_tokens': ['<|section|>']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)\n",
    "# model(model.dummy_inputs)  # Builds the model\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/workspace/Models/gpt2_recipe_basic\")\n",
    "model.save_weights('/workspace/Models/gpt2_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output(out):\n",
    "    return out#out.replace('<|startofrecipe|>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(filter_output(out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFOPTForCausalLM, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"facebook/opt-125m\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFOPTForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/opt_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-large', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2-large\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/gpt2-large_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs','rice,chocolate,lemon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)\n",
    "    print(\"###############################################\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistlGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882ca1a889314490bc45413e29b6a4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8268be5890143efb30b043de45c3075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fab30780ef2459f8b5fe374a963bcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f08a84f694e3aa0236cf23bc7632c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 10:46:00.730414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.743677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.743751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.744082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 10:46:00.745557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.745623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.745664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.776815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.776891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.776940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-13 10:46:00.776983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19676 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e7f3c1188f4d6d845b75a176402890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1784913 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416aecb9c1f544afa0756c95c7dd1e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/446229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 878596\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 219848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/distilgpt2_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs','chicken,rice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length-1,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)\n",
    "    print(\"####################\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 14:03:30.833242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 14:03:30.896232: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-13 14:03:31.265954: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-07-13 14:03:31.265986: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-07-13 14:03:31.265988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer, AutoConfig, TFOPTForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    # Load the GPT tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name, \n",
    "                                                bos_token='<|startofrecipe|>', \n",
    "                                                eos_token='<|endofrecipe|>',\n",
    "                                                unk_token='<|unknown|>', \n",
    "                                                pad_token='<|pad|>',\n",
    "                                                use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    context_length=128\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        name,\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_ctx=context_length,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    if name.find(\"gpt\") ==-1:\n",
    "        model = TFOPTForCausalLM(config)\n",
    "        model.load_weights(\"/workspace/Models/opt_recipe_basic_weights\")\n",
    "    else:\n",
    "        model = TFGPT2LMHeadModel(config)\n",
    "        model.load_weights(f\"/workspace/Models/{name}_recipe_basic_weights\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greet(Ingredients,model_name):\n",
    "    \n",
    "    prompt = create_prompt(Ingredients)\n",
    "    model, tokenizer = load_model(model_name)\n",
    "    pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    context_length=150\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    out = out.replace(\"ingredients:\", \"Ingredients:\")\n",
    "    return out.replace(\"directions:\", \"\\nDirections:\")\n",
    "\n",
    "        \n",
    "\n",
    "demo = gr.Interface(fn=greet, \n",
    "                    inputs=[\"text\", gr.Dropdown([\"distilgpt2\",\"gpt2\",\"gpt2-large\",\"facebook/opt-125m\"], label=\"Models\", info=\"Choose a Model!\")], \n",
    "                            outputs=\"text\")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
