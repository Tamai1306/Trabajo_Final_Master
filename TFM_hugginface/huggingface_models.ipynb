{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobar que la GPU esta disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar el dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset = datasets.load_dataset(\"recipe_nlg\", data_dir='/workspace/datasets/recipe_nlg')\n",
    "\n",
    "print(recipe_dataset)\n",
    "# Convertir los datos a dataframe de pandas\n",
    "recipe_dataset.set_format(type='pandas')\n",
    "df_recipe_dataset=recipe_dataset['train'][:]\n",
    "\n",
    "# Mostrar la cabecera del dataframe\n",
    "df_recipe_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acondicionamiento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar las columnas \"title\", \"ingredients\" y \"directions\" para tener la receta completa como un texto, los \"ner\" igual\n",
    "full_recipe=[]\n",
    "Ingredients=[]\n",
    "for row in range(df_recipe_dataset.shape[0]):\n",
    "    ingredients = \"\\n\".join(df_recipe_dataset['ner'][row])\n",
    "    directions = \"\\n\".join(df_recipe_dataset['directions'][row])\n",
    "    Ingredients.append('ingredients:\\n'+ingredients.lower())\n",
    "    full_recipe.append('<|startofrecipe|>ingredients:\\n'+ingredients.lower()+'\\ndirections:\\n'+directions.lower()+'<|endofrecipe|>')\n",
    "\n",
    "df_recipe_dataset['Full Recipe']=full_recipe\n",
    "df_recipe_dataset['Ingredients']=Ingredients\n",
    "# Eliminar columnas que no son Ãºtiles\n",
    "df_recipe_dataset=df_recipe_dataset.drop(['link', 'source','title','directions','ingredients','id','ner'], axis=1)\n",
    "\n",
    "# Visualizar el dataset acondicionado\n",
    "print(\"DATASET ACONDICIONADO:\")\n",
    "display(df_recipe_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar subsamplings del dataset acondicionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/workspace/datasets/clean_recipe_nlg/\"):\n",
    "    os.mkdir(\"/workspace/datasets/clean_recipe_nlg/\")\n",
    "df_recipe_dataset.iloc[:100,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_100_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:1000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_1000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:10000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_10000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:100000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_100000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.iloc[:1000000,:].to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_1000000_new.csv\", sep=\";\")\n",
    "df_recipe_dataset.to_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_full.csv\", sep=\";\")\n",
    "del df_recipe_dataset\n",
    "del recipe_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar dataset acondicionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe_dataset = pd.read_csv(\"/workspace/datasets/clean_recipe_nlg/recipe_nlg_full.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a Dataset de HuggingFace\n",
    "recipe_dataset = Dataset.from_pandas(df_recipe_dataset.sample(frac=1))\n",
    "\n",
    "# Dividir el dataset en train y test de forma aleatoria\n",
    "recipe_dataset_train_valid = recipe_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"DATASET:\")\n",
    "recipe_dataset = DatasetDict({\n",
    "    'train': recipe_dataset_train_valid['train'],\n",
    "    'valid': recipe_dataset_train_valid['test'],\n",
    "})\n",
    "print(recipe_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)\n",
    "#special_tokens_dict = {'additional_special_tokens': ['<|section|>']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)\n",
    "# model(model.dummy_inputs)  # Builds the model\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a dataset de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar y entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/workspace/Models/\"):\n",
    "    os.mkdir(\"/workspace/Models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/workspace/Models/gpt2_recipe_basic\")\n",
    "model.save_weights('/workspace/Models/gpt2_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar inferencia y generar la receta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar pipeline de generaciÃ³n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output(out):\n",
    "    return out#out.replace('<|startofrecipe|>', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la inferencia en base a los ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(filter_output(out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook OPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFOPTForCausalLM, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"facebook/opt-125m\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFOPTForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a dataset de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar y entrenar el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar pipeline de generaciÃ³n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/opt_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar inferencia y generar la receta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la inferencia en base a los ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2-large', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2-large\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a dataset de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar y entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-08, clipnorm=1.0)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/gpt2-large_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar inferencia y generar la receta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar pipeline de generaciÃ³n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs','rice,chocolate,lemon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la inferencia en base a los ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)\n",
    "    print(\"###############################################\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistlGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, GPT2Config, AutoTokenizer\n",
    "# Load the GPT tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', \n",
    "                                              bos_token='<|startofrecipe|>', \n",
    "                                              eos_token='<|endofrecipe|>',\n",
    "                                              unk_token='<|unknown|>', \n",
    "                                              pad_token='<|pad|>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n",
    "context_length=128\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"Full Recipe\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = recipe_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=recipe_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir a dataset de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"valid\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar y entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=5000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar pesos del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/workspace/Models/distilgpt2_recipe_basic_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar inferencia y generar la receta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar pipeline de generaciÃ³n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = ['butter,brown sugar,milk,eggs','chicken,rice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la inferencia en base a los ingredientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ing in ingredients:\n",
    "    prompt = create_prompt(ing)\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length-1,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    print(out)\n",
    "    print(\"####################\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer, AutoConfig, TFOPTForCausalLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ingredients):\n",
    "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
    "    ingredients = ingredients.strip().replace(',','\\n')\n",
    "    s = f\"ingredients:\\n{ingredients}\\n\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    # Load the GPT tokenizer.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name, \n",
    "                                                bos_token='<|startofrecipe|>', \n",
    "                                                eos_token='<|endofrecipe|>',\n",
    "                                                unk_token='<|unknown|>', \n",
    "                                                pad_token='<|pad|>',\n",
    "                                                use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    context_length=128\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        name,\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_ctx=context_length,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    if name.find(\"gpt\") ==-1:\n",
    "        model = TFOPTForCausalLM(config)\n",
    "        model.load_weights(\"/workspace/Models/opt_recipe_basic_weights\")\n",
    "    else:\n",
    "        model = TFGPT2LMHeadModel(config)\n",
    "        model.load_weights(f\"/workspace/Models/{name}_recipe_basic_weights\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar aplicaciÃ³n Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(Ingredients,model_name):\n",
    "    \n",
    "    prompt = create_prompt(Ingredients)\n",
    "    model, tokenizer = load_model(model_name)\n",
    "    pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    context_length=150\n",
    "    out = pipe(prompt,\n",
    "        max_length=context_length,\n",
    "        min_length=64,\n",
    "        penalty_alpha=0.9,\n",
    "        top_k=60,\n",
    "        pad_token_id=50260\n",
    "        )[0]['generated_text']\n",
    "    out = out.replace(\"ingredients:\", \"Ingredients:\")\n",
    "    return out.replace(\"directions:\", \"\\nDirections:\")\n",
    "\n",
    "        \n",
    "\n",
    "demo = gr.Interface(fn=greet, \n",
    "                    inputs=[\"text\", gr.Dropdown([\"distilgpt2\",\"gpt2\",\"gpt2-large\",\"facebook/opt-125m\"], label=\"Models\", info=\"Choose a Model!\")], \n",
    "                            outputs=\"text\")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
